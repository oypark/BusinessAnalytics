{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3강. 텍스트 마이닝의 이론과 실제\n",
    "\n",
    "***\n",
    "---\n",
    "\n",
    "## 1. 텍스트 마이닝의 이해\n",
    "\n",
    "### 1) 텍스트 마이닝 (Text Mining)이란?\n",
    "* Wikipedia: the process of **deriving high-quality information from text.**\n",
    "* High-quality information is typically derived through the devising of **patterns and trends** through means such as **statistical pattern learning.**\n",
    "* The overarching goal is, essentially, **to turn text into data for analysis**, via application of natural language processing (NLP) and analytical methods.\n",
    "\n",
    ">* 일정한 길이 (sparse or dense) 의 vector로 변환\n",
    ">* 변환된 vector에 머신러닝 (딥러닝) 기법을 적용\n",
    "\n",
    "### 2) 텍스트 마이닝의 이해를 위한 기본요구지식\n",
    "* 자연어 처리\n",
    "  * 강의자료에서 정리\n",
    "* 통계학 & 선형대수\n",
    "  * 조건부 확률, 벡터, 선형결합 …\n",
    "* 머신러닝\n",
    "  * 회귀분석의 개념\n",
    "  * 머신러닝의 다양한 기법(나이브 베이즈,\n",
    "* 딥러닝\n",
    "  * 딥러닝의 개념\n",
    "  * 딥러닝의 다양한 기법(CNN, RNN, …)\n",
    "  \n",
    "### 3) 텍스트 마이닝 방법\n",
    "* NLP(Natural Language Processing) 기본도구\n",
    ">* Tokenize, stemming, lemmatize\n",
    ">* Chunking\n",
    ">* BOW, TFIDF – **sparse representation**\n",
    "* 머신러닝(딥러닝)\n",
    ">* Naïve Bayes, Logistic egression, Decision tree, SVM\n",
    ">* Embedding(Word2Vec, Doc2Vec) – **dense representation**\n",
    ">* RNN(LSTM), Attention, Transformer\n",
    "\n",
    "### 4) 텍스트 마이닝 단계\n",
    "  <img src=\"3-1.png\">\n",
    "  \n",
    "### 5) 텍스트 마이닝 적용 분야\n",
    "* Document classification\n",
    "  * Sentiment analysis, classification\n",
    "* Document generation\n",
    "  * Q&A, summarization, translation\n",
    "* Keyword extraction\n",
    "  * tagging/annotation\n",
    "* Topic modeling\n",
    "  * LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation)\n",
    "  \n",
    "### 6) 텍스트 마이닝 도구 - 파이썬\n",
    "* NLTK\n",
    "  * 가장 많이 알려진 NLP 라이브러리\n",
    "* Scikit Learn\n",
    "  * 머신러닝 라이브러리\n",
    "  * 기본적인 NLP, 다양한 텍스트 마이닝 관련 도구 지원\n",
    "* Gensim\n",
    "  * Word2Vec으로 유명\n",
    "  * sklearn과 마찬가지로 다양한 텍스트 관련 도구 지원\n",
    "* Keras\n",
    "  * RNN, seq2seq 등 딥러닝 위주의 라이브러리 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 마이닝 방법론\n",
    "\n",
    "### 텍스트 마이닝 기본도구\n",
    "* 목적: document, sentence 등을 sparse vector로 변환\n",
    ">* Tokenize - Text normalization - POS-tagging - Chunking\n",
    ">* Tokenize - BOW/TFIDF\n",
    "\n",
    "#### 1) Tokenize\n",
    "* 대상이 되는 문서/문장을 최소 단위로 쪼갬\n",
    "\n",
    "#### 2) Text normalization\n",
    "* 최소 단위를 표준화\n",
    "\n",
    "#### 3) POS-tagging\n",
    "* 최소 의미단위로 나누어진 대상에 대해 품사를 부착\n",
    "\n",
    "#### 4) Chunking\n",
    "* POS-tagging의 결과를 명사구, 형용사구, 분사구 등과 같은 말 모듬으로 다시 합치는 과정\n",
    "\n",
    "#### 5) BOW, 6) TFIDF\n",
    "* tokenized 결과를 이용하여 **문서를 vector로 표현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Tokenize\n",
    "* Document를 Sentence의 집합으로 분리\n",
    "* Sentence를 **Word의 집합**으로 분리\n",
    "* 의미 없는 문자 등을 걸러 냄\n",
    "\n",
    "#### 영어 vs. 한글\n",
    "* **영어는 공백(space) 기준으로 비교적 쉽게 tokenize 가능**\n",
    "* **한글은 구조상 형태소(morpheme) 분석이 필요**\n",
    ">* 복합명사, 조사, 어미 등을 분리해내는 작업이 필요\n",
    ">* 영어에 비해 어렵고 정확도 낮음\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Text normalization\n",
    "* **동일한 의미의 단어**가 다른 형태를 갖는 것을 보완\n",
    "* **다른 형태**의 단어들을 **통일시켜 표준단어로 변환**\n",
    "\n",
    "#### (1) Stemming (어간 추출)\n",
    "* 단수 – 복수, 현재형 – 미래형 등 단어의 다양한 변형을 하나로 통일\n",
    "* 의미가 아닌 **규칙에 의한 변환**\n",
    "* 영어의 경우, Porter stemmer, Lancaster stemmer 등이 유명\n",
    "\n",
    "> **Stemming 예제 - Porter Stemmer**\n",
    ">* **원문**: \"This was not the map we found in Billy Bones's chest, but an\n",
    "accurate copy, complete in all things\"\n",
    ">* **tokenize 결과**: **'This', 'was'**, 'not', 'the', 'map', 'we', 'found', 'in', **'Billy'**, 'Bones', \"'s\", 'chest', ',', 'but', 'an', **'accurate', 'copy'**, ',', **'complete'**, 'in', 'all', 'things’\n",
    ">* **stemming 결과**: **'thi', 'wa'**, 'not', 'the', 'map', 'we', 'found', 'in', **'billi'**, 'bone', \"'s\", 'chest', ',', 'but', 'an', **'accur', 'copi'**, ',', **'complet'**, 'in', 'all', 'thing’\n",
    "> **문제점**: this → thi, was → wa, accurate → accur 등 이상한 단어로 변환. 이는 사전이 아닌 알고리즘(규칙)에 의해 변환하기 때문\n",
    "\n",
    "#### (2) Lemmatization (표제어 추출)\n",
    "* **사전을 이용**하여 단어의 원형을 추출\n",
    "* 품사(part-of-speech)를 고려\n",
    "* 영어의 경우, 유명한 어휘 데이터베이스인 WordNet을 이용한 WordNet lemmatizer가 많이 쓰임\n",
    "\n",
    "> **Lemmatization 예제 - WordNetLemmatizer**\n",
    ">* **원문**: \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things\"\n",
    ">* **tokenize 결과**: 'This', **'was'**, 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', **'Bones'**, \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', **'things'**\n",
    ">* **lemmatization 결과**: 'This', **'wa'**, 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', **'Bones'**, \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', **'thing'**\n",
    "> was는 품사를 모르기 때문에 형태적으로 접근. things는 사전에 근거하여 thing으로 변환. 나머지 단어는 사전의 원형을 유지\n",
    "\n",
    "---\n",
    "\n",
    "### 3) POS-tagging\n",
    "* 토큰화와 정규화 작업을 통해 나누어진 **형태소**(의미를 가지는 최소단위)**에 대해 품사를 결정하여 할당**하는 작업\n",
    "*  동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 **문맥을 파악해야 함**\n",
    "* Text-to-speech(텍스트를 읽어주는 시스템)에서도 각 단어에 대해 올바른 발음을 하기 위해 품사 태깅을 이용\n",
    "* POS Tagging은 형태소 분석으로 번역되기도 하는데, **형태소 분석은** 주어진 텍스트(원시말뭉치)를 형태소 단위로 나누는 작업을 포함하므로 **앞의 토큰화, 정규화 작업에 품사 태깅을 포함**한 것으로 보는 것이 타당\n",
    "\n",
    "#### 한글 형태소 분석기 예제\n",
    "* 한글 형태소 분석기 도구 - 파이썬 KoNLPy: <http://konlpy.org/ko/v0.4.3/>\n",
    "* 토큰화, 정규화, POS-tagging이 모두 이루어짐\n",
    "* 예시: 주택 문제의 경우 제 나이가 아직 젊으니까 가능성이 많지요.\n",
    "  <img src=\"3-2.png\" width=\"50%\">\n",
    "  \n",
    "---\n",
    "  \n",
    "### 4) Chunking\n",
    "* **Chunk는** 언어학적으로 **말모듬**을 뜻하며, 명사구, 형용사구, 분사구 등과 같이 주어와 동사가 없는 **두 단어 이상의 집합인 구(phrase)**를 의미\n",
    "* Chunking은 주어진 텍스트에서 이와 같은 **chunk를 찾는 과정**\n",
    "* 즉, 형태소 분석의 결과인 각 형태소들을 서로 겹치지 않으면서 **의미가 있는 구로 묶어나가는 과정**임\n",
    "* 텍스트로부터 **Information Extraction(정보추출)을 하기 위한 전단계**로 보거나 **혹은 Information Extraction에 포함되기도 함**\n",
    "\n",
    "#### 개체명 인식(Named Entity Recognition, NER)\n",
    "* **개체명**(Named Entity)은 기관, 단체, 사람, 날짜 등과 같이 **특정 정보에 해당하는 명사구**를 의미\n",
    "* 따라서 NER은 텍스트로부터 뭔가 **의미 있는 정보를 추출**하기 위한 방법으로 사용\n",
    "* 예를 들어 “James is working at Disney in London”이라는 문장이 있을 때, **James는 PERSON, Disney는 ORGANIZATION, London은 GPE(지리적인 장소)임을 알아내는 작업**.\n",
    "> **관계 인식 (Relation Extraction)**\n",
    ">* NER에 의해 추출된 개체명들을 대상으로 그들 간의 관계를 추출\n",
    "하는 작업\n",
    ">* 특정 건물이 특정 장소에 위치하는 관계와 같은 지식을 텍스트\n",
    "로부터 추출할 때 사용\n",
    ">* 위 NER 예에서 **in을 이용해 Disney는 London에 위치한다는 관계\n",
    "를 추출**\n",
    "\n",
    "---\n",
    "\n",
    "### 5) BOW (Bag of Words)\n",
    "* **Vector Space Model**\n",
    "  * 문서를 bag of words로 표현\n",
    "  * 단어가 쓰여진 **순서는 무시**\n",
    "  * 모든 문서에 **한번 이상 나타난 단어들에 대해 유(1)/무(0)로 문서를 표현**\n",
    "* **count vector**\n",
    "  * 단어의 유/무 대신 **단어가 문서에 나타난 횟수**로 표현\n",
    "  * **count가 weight로 작용**\n",
    "  <img src=\"3-2.png\" width=\"50%\">\n",
    "* BOW의 활용\n",
    "  <img src=\"3-6.png\">\n",
    "---\n",
    "  \n",
    "### 6) TFIDF(Term Frequency - Inverse Document Frequency)\n",
    "* **count vector의 문제점**\n",
    "  * **많은 문서에 공통적으로 나타난 단어는 중요성이 떨어지는 단어일 가능성이 높음** ex) the, a, …\n",
    "* **TFIDF**\n",
    "> 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림\n",
    ">* tf(d, t): 문서 d에 단어 t가 나타난 횟수, count vector와 동일, 로그스케일 등 다양한 변형이 있음\n",
    ">* df(t): 전체 문서 중에서 단어 t를 포함하는 문서의 수\n",
    ">* idf(t): df(t)의 역수를 바로 쓸 수도 있으나, 여러가지 이유로 로그스케일과 스무딩을 적용한 공식을 사용. log(n/(1+df(t)), n은 전체 문서 수\n",
    "* **TFIDF 계산 예제**\n",
    "  <img src=\"3-4.png\">\n",
    "* **TFIDF의 변형**: 다양한 변형이 있음\n",
    "  <img src=\"3-5.png\">\n",
    "* **TFIDF를 이용한 유사도 계산**\n",
    "  * TFIDF Matching score: TFIDF vector의 내적을 이용\n",
    "  <img src=\"3-7.png\">\n",
    "  * Cosine Similarity: vector의 방향에 대한 유사도\n",
    "  <img src=\"3-8.png\">\n",
    "  \n",
    "#### Text Classification(문서분류) with BOW/TFIDF\n",
    "* Naïve Bayes\n",
    "* Logistic regression\n",
    "  * Ridge regression\n",
    "  * Lasso regression\n",
    "* Decision tree\n",
    "  * Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "---\n",
    "\n",
    "### (1) Naïve Bayes\n",
    "\n",
    "* Wikipedia: a popular (baseline) method for **text categorization**, the problem of judging documents as belonging to one category or the other (such as **spam or legitimate**, sports or politics, etc.) with **word frequencies (count vector)** as the features.\n",
    "* **(x1,… xn) 의 단어집합으로 이루어진 문서가 분류 Ck에 속할 확률**\n",
    "  <img src=\"3-9.png\">\n",
    "  * 위 확률이 가장 큰 Ck로 분류\n",
    "* **Naïve Bayes 예제**\n",
    "<http://bcho.tistory.com/m/1010>\n",
    "  <img src=\"3-10.png\">\n",
    "  <img src=\"3-11.png\">\n",
    "  \n",
    "  \n",
    "### (2) Logistic Regression\n",
    "* 분류를 위한 **회귀분석**\n",
    "  * **종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용**\n",
    "  * **종속 변수가 범주형 데이터**를 대상으로 하며, 입력 데이터가 주어졌을 때 해당 데이터의 **결과가 특정 분류로 나뉘기 때문에 일종의 분류 (classification) 기법에 해당**\n",
    "  \n",
    "* **텍스트 마이닝에서의 문제**\n",
    "  * **추정해야 할 계수가 vector의 크기(단어의 수)만큼 존재**하므로, **과적합이 발생하기 쉽고 많은 데이터 셋이 필요**\n",
    "  * 그럼에도 불구하고 **잘 작동하는 편**\n",
    "  * **정규화(regulation)을 이용해 과적합 해결 노력**\n",
    "\n",
    "#### Ridge and Lasso Regression\n",
    "* **릿지 회귀 (Ridge regression)**\n",
    "  * 목적함수에 추정할 계수(parameter)에 대한 L2 norm(규제항)을 추가하여 모형의 과적합을 방지\n",
    "  <img src=\"3-12.png\" width=\"50%\">\n",
    "* **라쏘 회귀 (Lasso regression)**\n",
    "  * L1 norm을 규제항으로 사용함으로써 0에 가까운 계수를 0으로 만들어 **영향을 거의 미치지 않는 단어들을 제외**\n",
    "  * 남은 단어들로 분류의 이유에 대해 설명이 가능하다는 장점이 있음\n",
    "  * feature selection의 효과가 있음\n",
    "\n",
    "\n",
    "### (3) 문서분류의 활용 - Sentiment Analysis 감정분석\n",
    "* 네이버 지식백과\n",
    "   * **소비자의 감성과 관련된 텍스트 정보를 자동으로 추출하는 텍스트 마이닝(Text Mining) 기술의 한 영역**. 문서를 작성한 사람의 감정을 추출해 내는 기술로 문서의 주제보다 어떠한 감정을 가지고 있는가를 판단하여 분석한다.\n",
    "* Wikipedia – 보다 포괄적인 정의\n",
    "  * **Sentiment analysis** (sometimes known as **opinion mining or emotion AI**) refers to the use of **natural language processing, text analysis, computational linguistics, and biometrics** to systematically identify, extract, quantify, and study **affective states** and subjective information.\n",
    "* Demonstration: Sentence-level Sentiment\n",
    "<http://nlp.stanford.edu:8080/sentiment/rntnDemo.html>\n",
    "  <img src=\"3-13.png\">\n",
    "\n",
    "#### 한글 감성분석 예제\n",
    "* 학습을 위한 데이터 예: label이 0이면 부정, 1이면 긍정\n",
    "  <img src=\"3-14.png\" width=\"80%\">\n",
    "* 학습 방법\n",
    "  * 리뷰를 BOW로 변환 후 input으로 쓰고, label을 target으로 하여 학습\n",
    "  * 나이브베이즈, 로지스틱 회귀분석, SVM 등 다양한 방법의 사용이 가능\n",
    "  * 새로운 리뷰에 대해 긍정/부정을 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텍스트 마이닝의 문제점\n",
    "\n",
    "### 1) Curse of Dimensionality\n",
    "* 차원의 저주\n",
    "  * **Extremely sparse data**\n",
    "  * 각 데이터 간의 거리가 너무 멀게 위치\n",
    "  \n",
    "  <img src=\"3-15.png\" width=\"80%\">\n",
    "* 해결방법\n",
    "  * 더 많은 데이터 공간(?)\n",
    "  * **Dimension reduction(차원축소)**\n",
    ">* feature selection\n",
    ">* feature extraction\n",
    "\n",
    "### 2) 단어 빈도의 불균형\n",
    "* **Zipf’s law(멱법칙)**\n",
    "  * **극히 소수의 데이터가 결정적인 영향**을 미치게 됨\n",
    "    <img src=\"3-16.png\" width=\"50%\">\n",
    "* 해결방안\n",
    "  * feature selection\n",
    "  >* 빈도 높은 단어를 삭제\n",
    "  >* 심한 경우 50% 삭제\n",
    "  * Boolean BOW 사용\n",
    "  >* 1이상이면 1로 변환\n",
    "  * log 등의 함수를 이용해 weight를 변경\n",
    "  \n",
    "### 3) 단어가 쓰인 순서정보의 손실\n",
    "* 통계에 의한 의미 파악 vs. 순서에 의한 의미 파악\n",
    "* **Loss of sequence information**\n",
    "  * 단어들의 순서 – context가 중요\n",
    "  * 특히 번역과 같은 sequence-to-sequence 문제에서 매우 중요\n",
    "* 해결방안\n",
    "  * **n-gram**: 부분적 해결, 주로 classification 문제에서 유용\n",
    "  * **Deep learning**: RNN, Attention, Transformer, BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문제 해결을 위한 방안\n",
    "\n",
    "### Dimensionality Reduction (차원 축소)\n",
    "* 차원의 저주를 해결하기 위한 노력\n",
    "#### 1) Feature selection\n",
    "  * Manual, Regularization(Lasso)\n",
    "#### 2) Feature extraction\n",
    "  * PCA, LSA(SVD)\n",
    "#### 3) Embedding\n",
    "  * Word embedding, Document embedding\n",
    "  \n",
    "#### 4) Deep Learning\n",
    "  * RBM, Autoencoder\n",
    "  \n",
    "---\n",
    "\n",
    "### 1) Feature selection\n",
    "* Wiki: feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant\n",
    "  <img src=\"3-17.png\" width=\"80%\">\n",
    "  <https://stats.stackexchange.com/questions/2691/ma>\n",
    "  \n",
    "### 2) Feature extraction\n",
    "* PCA: 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변환함으로써 차원을 축소\n",
    "\n",
    "#### (1) 주성분 분석 (Principal Component Analysis)\n",
    "* 선형결합 (linear combination)\n",
    "  * X (데이터, nxp): n(sample의 수), p(변수의 수)\n",
    "  * Xi: i번째 feature에 대한 크기 n의 vector\n",
    "  * Z(X의 선형결합으로 이루어진 새로운 변수, nxp) = XVT\n",
    "  * V^T: 새로운 축 (pxp)  공분산행렬의 고유벡터로 구성\n",
    "* 공분산행렬\n",
    "  <img src=\"3-18.png\" width=\"80%\">\n",
    "* 고유값이 큰 순서대로 고유벡터를 정렬하여 차원 선택\n",
    "* 선택된 고유벡터와 X의 선형결합으로 차원 축소\n",
    "\n",
    "#### (2) LSA(Latent Semantic Analysis)\n",
    "* a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by **producing a set of concepts related to the documents and terms**. LSA assumes that **words that are close in meaning will occur in similar pieces of text**(the distributional hypothesis)\n",
    "\n",
    "#### SVD(Singular Vector Decomposition) 특이값 분해\n",
    "* 주어진 dtm(document term matrix, A)을 A=UΣVT의 형태로 분해\n",
    "* document 수:m, term 수: n\n",
    "* U, V는 각각 열벡터가 singular vector로, 서로 직교하는 성질을 가짐. UTU = I, VTV = I\n",
    "* Σ는 대각행렬로 제곱하면 PCA에서의 고유값이 됨\n",
    "* A=UΣV^T\n",
    "  <img src=\"3-19.png\" width=\"80%\">\n",
    "* Truncated SVD\n",
    "  * Σ 행렬의 대각원소(특이값) 가운데 상위 t개만 골라냄\n",
    "  * **t는 잠재의미군 중 비중이 큰 것부터 차례대로 골라낸 것**\n",
    "  * A’은 축소된 U, Σ, V로 복원한 값으로 A에 근사한 값\n",
    "  <img src=\"3-20.png\" width=\"80%\">\n",
    "\n",
    "#### 잠재의미분석\n",
    "* Ut(m, t), Σ(t, t), Vt^T(t, n)\n",
    "  * m: sample의 수, n: 변수의 수, t: 축소한 잠재의미의 차원\n",
    "  * Vt: 원래 A의 공분산행렬의 고유벡터로 이루어진 행렬\n",
    "* UtΣ(m, t)\n",
    "  * 각 sample에 대해서 n  t로 변수가 줄어든 data set\n",
    "  * 각 문서에서 잠재의미 별 비중(?)\n",
    "* Σ Vt^T(t, n)\n",
    "  * 각 변수에 대해서 sample의 차원을 t로 줄임\n",
    "  * 각 단어에서 잠재의미 별 비중(?)\n",
    "  <img src=\"3-21.png\" width=\"80%\">\n",
    "* 잠재의미분석의 활용\n",
    "  * 문서 간의 유사도\n",
    "  >* Count vector나 TFIDF에 cosine similarity를 직접 적용하는 경우, 물리적인 단어로만 유사도를 측정하게 됨\n",
    "  >* 잠재의미분석을 활용하면 직접적인 단어가 아니라 의미적으로 유사한(문서에서 함께 많이 등장한) 단어들로 유사도를 측정하는 것이 가능할 것으로 기대\n",
    "  * 단어 간의 유사도\n",
    "  >* 마찬가지로 주어진 문서 집합에서 단어들이 어떤 유사도를 가지는 지 볼 수 있음\n",
    "  \n",
    "#### (3) Topic Modeling\n",
    "* Topic models\n",
    "  * Documents are mixtures of topics\n",
    "  * **A topic is a probability distribution over words**\n",
    "  * A topic model is a generative model for documents: Different documents can be produced by picking words from a topic depending on the weight given to the topic\n",
    "  * **Infer the set of topics** that were responsible for generating **a collection of documents**\n",
    "  * The goal is to find the best set of latent variables that can explain the observed data(observed words in documents)\n",
    "* Determining the number of topics\n",
    "  * Too many topics: Uninterpretable topics\n",
    "  * Too few topics: Very broad topics\n",
    "* Topic Modeling의 원리\n",
    "  <img src=\"3-23.png\" width=\"80%\">\n",
    "* Topic Modeling 활용사례\n",
    "  <img src=\"3-24.png\" width=\"80%\">\n",
    "* Example of Topics - \"My Love from the Star\" 별그대\n",
    "  <img src=\"3-25.png\" width=\"80%\">\n",
    "* Topic Trends - same as upper example\n",
    "  <img src=\"3-26.png\" width=\"80%\">\n",
    "\n",
    "#### (4) Latent Dirichlet Allocation\n",
    "<https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>\n",
    ">* α is the parameter of the Dirichlet prior on the per-document\n",
    "topic distributions,\n",
    ">* β is the parameter of the Dirichlet prior on the per-topic word\n",
    "distribution,\n",
    ">* is the topic distribution for document i,\n",
    ">* is the word distribution for topic k,\n",
    ">* is the topic for the jth word in document i,\n",
    ">* is the specific word.\n",
    "  <img src=\"3-22.png\" width=\"80%\">\n",
    "  \n",
    "---\n",
    "\n",
    "### 3) Embedding\n",
    "\n",
    "#### (1) Word Embedding\n",
    "* **단어에 대한 vector의 dimension reduction이 목표**\n",
    "단어의 표현\n",
    "* **Term-Document Matrix에서 Document 별 count vector: 문서가 추가되면 일반화가 어려움**\n",
    "* **one-hot-encoding**: extremely sparse\n",
    "  * 각 단어를 모든 문서에서 사용된 단어들의 수 길이의 벡터로 표현, 심한 경우 길이 30만의 벡터 중에서 하나만 1인 sparse vector가 됨\n",
    "  <img src=\"3-26.png\" width=\"80%\">\n",
    "* **Word embedding**\n",
    "  * one-hot-encoding으로 표현된 단어를 **dense vector**로 변환\n",
    "  * 변환된 vector를 이용하여 학습\n",
    "  * 최종목적에 맞게 학습에 의해 vector가 결정됨\n",
    "  * 학습목적 관점에서의 단어의 의미를 내포\n",
    "\n",
    "* BOW vs. Word Embedding\n",
    "  <img src=\"3-28.png\">\n",
    "\n",
    "#### Word Embedding을 이용한 문서 분류\n",
    "* BOW와는 다른 관점의 문서 표현\n",
    "  * document: 제한된 maxlen 개의 word sequence (앞이나 뒤를 잘라냄)\n",
    "  * word: one-hot-vector에서 저차원(reduced_dim)으로 embedding된 dense vector\n",
    "  * 즉 document는 (maxlen, reduced_dim)의 2차원 행렬로 표현\n",
    "* 단순한 분류모형 (sequence 무시)\n",
    "  * (maxlen, reduced_dim) 차원의 document maxlen*'reduced_dim 차원으로 펼쳐서 분류모형에 적용\n",
    "  \n",
    "#### (2) Word2Vec\n",
    "* 문장에 나타난 단어들의 순서를 이용해 word embedding을 수행\n",
    "  * CBOW: 주변단어들을 이용해 다음 단어를 예측\n",
    "  * Skip-gram: 한 단어의 주변단어들을 예측\n",
    "  <https://www.researchgate.net/figure/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models_fig1_281812760>\n",
    "  <img src=\"3-29.png\">\n",
    "\n",
    "#### Word2Vec의 학습방법\n",
    "* sliding window를 이용한 학습set 구성\n",
    "  * 주어진 주변 단어들을 입력했을때, target word의 확률이 높아지도록 학습 혹은 그 반대\n",
    "* embedding vector\n",
    "  * input이 one-hot vector이므로 W가 embedding vector의 집합이 됨\n",
    "  <img src=\"3-30.png\" width=\"50%\">\n",
    "  \n",
    "#### Word2Vec의 의미\n",
    "* 단어의 위치에 기반하여 의미를 내포하는 vector 생성\n",
    "  * 비슷한 위치에 나타나는 단어들은 비슷한 vector를 가지게 됨\n",
    "  * 단어 간의 유사성을 이용하여 연산이 가능\n",
    "  <https://www.tensorflow.org/im>\n",
    "  <img src=\"3-31.png\">\n",
    "  \n",
    "#### (3) ELMo (Embeddings from Language Model)\n",
    "* **사전 훈련된 언어 모델을 사용하는 워드 임베딩 방법론**\n",
    "* 이전의 대표적인 임베딩 기법인 Word2Vec이나 GloVe 등이 동일한 단어가 문맥에 따라 전혀 다른 의미를 가지는 것을 반영하지 못하는 것에 비해, ELMo는 이러한 **문맥을 반영**하기 위해 개발된 워드 임베딩 기법\n",
    "* 문맥의 파악을 위해 **biLSTM(뒷쪽 Deep Learning 파트에 자세히 설명)으로 학습된 모형을 이용**\n",
    "  <img src=\"3-32.png\" width=\"50%\">\n",
    "  \n",
    "#### (4) Transfer Learning(전이 학습)\n",
    "* Wiki: storing knowledge gained while solving one problem and applying it to a different but related problem\n",
    "* 텍스트 마이닝에서의 전이학습\n",
    "  * **feature level**: 단어에 대한 dense vector(word embedding)를 새로 학습하지 않고 학습된 vector를 그대로 가져다 씀\n",
    "  * **model level**: word embedding과 모형 전체를 가져다 학습\n",
    "  * Word2Vec, Glove, ELMo 등의 사전학습된 word embedding이 전이학습에 많이 사용됨\n",
    "  \n",
    "#### (5) Document Embedding\n",
    "* Word2Vec은 word에 대해 dense vector를 생성하지만, document vector는 여전히 sparse\n",
    "* Word2Vec 모형에서 주변단어들에 더하여 document의 고유한 vector를 함께 학습함으로써 document에 대한 dense vector를 생성\n",
    "*  이 dense vector를 이용해 매칭, 분류 등의 작업 수행\n",
    "  <img src=\"3-33.png\" width=\"50%\">\n",
    "  <https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e>\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Deep Learning\n",
    "\n",
    "#### (1) RBM (Restricted Boltzmann Machine)\n",
    "* 사전학습 목적으로 개발\n",
    "  * G. Hinton에 의해 제안, 차원을 변경하면서 원래의 정보량 유지가 목적\n",
    "  * 정보량을 물리학의 에너지 함수로 표현\n",
    "* Deep NN의 vanishing gradient 문제 해결을 위해 제안\n",
    "  * batch normalization, Dropout, ReLU 등의 기법으로 인해 문제가 해결되면서 지금은 많이 쓰이지 않음\n",
    "* 사전학습을 통한 차원 축소에 사용 가능\n",
    "  <img src=\"3-34.png\" width=\"50%\">\n",
    "  \n",
    "#### (2) Autoencoder\n",
    "* RBM과 유사한 개념\n",
    "  * encoder로 차원을 축소하고 decoder로 다시 복원했을 때, 원래의 X와 복원한 X’이 최대한 동일하도록 학습\n",
    "* 작동방식은 PCA와 유사\n",
    "  * 데이터에 내재된 일정한 구조 – 연관성을 추출\n",
    "    <img src=\"3-35.png\" width=\"50%\">\n",
    "    \n",
    "#### Context(sequence)의 파악\n",
    "* N-gram\n",
    "  * 문맥(context)를 파악하기 위한 전통적 방법\n",
    "  * bi-gram, tri-gram, …\n",
    "  * 대상이 되는 문자열을 하나의 단어 단위가 아닌, **두개 이상의 단위로 잘라서 처리**\n",
    "* 딥러닝 – RNN\n",
    "  * **문장을 단어들의** sequence 혹은 **series로 처리**\n",
    "  * **뒷 단어에 대한 hidden node가 앞 단어의 hidden node 값에도 영향을 받도록 함**\n",
    "  * 그 외에도 단어들 간의 관계를 학습할 수 있는 모형을 고안\n",
    "  \n",
    "#### (3) N-gram\n",
    "* “ The future depends on what we do in the present”\n",
    "* **Unigram\n",
    "  * The, future, depends, on, what, we, do, in, the, present\n",
    "* **Bi-gram\n",
    "  * ‘The future’, ‘future depends’, ‘depends on’, ‘on what’, …\n",
    "* **Tri-gram\n",
    "  * ‘The future depends’, ‘future depends on’, ‘depends on what’, …\n",
    "* **보통 unigram에 bi-gram, tri-gram을 추가하면서 feature의 수를 증가시켜 사용**\n",
    "  * **문맥 파악에 유리하나, dimension이 더욱 증가**\n",
    "\n",
    "#### (4) 딥러닝과 텍스트마이닝 - RNN\n",
    "* Sequence 정보를 기억할 수 있는 방법은?\n",
    "  * X는 embedding된 word들을 순서대로 나열한 것\n",
    "  * ex) X1: The, X2: future, X3: depends, X4: on, …\n",
    "  * hidden node가 X 뿐만 아니라 **이전 hidden node로부터도 입력을 받음**\n",
    "  <img src=\"3-36.png\"> \n",
    "  <https://en.wikipedia.org/wiki/Recurrent_neural_network>\n",
    "* Text Classification with RNN\n",
    "  <img src=\"3-37.png\"> \n",
    "\n",
    "#### LSTM (Long Short Term Memory)\n",
    "* RNN의 문제\n",
    "  * 문장이 길수록 층이 깊은 형태를 갖게 됨 → 경사가 소실되는 문제 발생 → 앞부분의 단어 정보가 학습되지 않음\n",
    "* LSTM: 직통 통로를 만들어 RNN의 문제를 해결\n",
    "<https://colah.github.io/posts/2015-08-Understanding-LSTMs/>\n",
    "  <img src=\"3-38.png\">\n",
    "\n",
    "#### Bi-LSTM\n",
    "* 단방향 LSTM의 문제\n",
    "  * 단어 순서가 갖는 문맥 정보가 한 방향으로만 학습된다.\n",
    "  * 자신의 뒤에 오는 단어에 의해 영향을 받는 경우, 학습이 되지 않음\n",
    "* Bi-LSTM\n",
    "  * 양방향으로 LSTM을 구성하여 두 결과를 합침\n",
    "  * 양방향 순서를 모두 학습\n",
    "<https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66>\n",
    "  <img src=\"3-39.png\">\n",
    "\n",
    "#### (5) 합성곱 신경망(Convolutional Neural Networks, CNN)\n",
    "* CNN은 원래 이미지 처리를 위해 개발된 신경망으로, 현재는 인간의 이미지 인식보다 더 나은 인식 성능을 보이고 있음.\n",
    "* 그러나 CNN이 주변 정보를 학습한다는 점을 이용하여 텍스트의 문맥을 학습하여 문서를 분류하는 연구가 처음 있었으며, 의외로 뛰어난 성능을 보이게 되면서 자연어 처리에서의 활용분야가 넓어지게 됨.\n",
    "* CNN은 합성곱층(conolution layer)와 풀링층(pooling)으로 구성되며, 합성곱층은 2차원 이미지에서 특정 영역의 특징을 추출하는 역할을 하는데, 이는 연속된 단어들의 특징을 추출하는 것과 유사한 특성이 있음.\n",
    "* 아래 그림은 전형적인 CNN의 구조를 보여주며, 합성곱층과 풀링층이 번갈아가면서 이미지의 특징을 단계적으로 추출하고, 마지막에 분류기를 통해 이미지를 판별하는구조를 보여줌.\n",
    "  <img src=\"3-40.png\">\n",
    "<https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets>\n",
    "\n",
    "#### CNN을 이용한 문서 분류\n",
    "* 아래 그림은 CNN을 이용한 문서분류 모형을 보여줌. 이미지와 달리 텍스트는 단어들의 1차원 시퀀스로 표현되므로 1D CNN모형을 사용함. 단어 시퀀스에 대해 CNN의 필터는 1차원으로만 적용되고 이렇게 텍스트의 특징을 추출한 결과를 마찬가지로 분류기에 넣어서 문서를 판별.\n",
    "  <img src=\"3-41.png\">\n",
    "\n",
    "#### (6) Sequence-to-sequence\n",
    "* 지금까지는 입력은 sequence, 출력은 하나의 값인 경우가 일반적이었으나, 번역, chat-bot, summarize등은 출력도 sequence가 되어야 함\n",
    "* encoder, decoder의 구조를 가짐\n",
    "<https://github.com/farizrahman4u/seq2seq>\n",
    "  <img src=\"3-42.png\">\n",
    "\n",
    "#### Attention\n",
    "  <img src=\"3-43.png\">\n",
    "* Attention을 이용한 번역 예\n",
    "  * 아래 그림은 “I am a student”를 번역하는 과정에서, 첫 단어 ‘Je’가 앞 단어들에 대해 attention으로 연결되어 있으며, 이 단어의 생성에 ‘I’가 중요한 영향을 미치고 있음을 보여줌.\n",
    "  <img src=\"3-44.png\">\n",
    "  <https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129>\n",
    "  \n",
    "#### Transformer (Self-attention)\n",
    "* 입력 단어들끼리도 상호연관성이 있는 것에 착안\n",
    "  * 즉 입력 → 출력으로의 attention 외에 입력 단어들 간의 attention, 입력 + 출력 → 출력으로의 attention을 추가\n",
    "  <img src=\"3-45.png\">\n",
    "  <https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html>\n",
    "* encoder와 decoder가 서로 다른 attention 구조를 사용\n",
    "  * multi-head attention vs. masked multi-head attention\n",
    "* RNN이 사라지고 self-attention이 이를 대신\n",
    "  <img src=\"3-46.png\" width=\"80%\">\n",
    "\n",
    "#### BERT(Bidirectional Encoder Representations form Transformer)\n",
    "* 양방향 transformer 인코더를 사용\n",
    "  * transformer에 기반한 OpenAI GPT와의 차이\n",
    "* transfer learning에서 feature + model을 함께 transfer하고 fine tuning을 통해서 적용하는 방식을 선택\n",
    "* 거의 모든 분야에서 top score를 기록\n",
    "  <img src=\"3-47.png\">\n",
    "* BERT 참고\n",
    "  <img src=\"3-48.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
